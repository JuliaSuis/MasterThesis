{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series\n",
    "\n",
    "import math\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import statsmodels\n",
    "import sklearn\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 25, 16\n",
    "\n",
    "import theano\n",
    "import tensorflow\n",
    "import keras\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../', sep='\\t', index_col=0, parse_dates=True)\n",
    "df = df.sort_index()\n",
    "\n",
    "def create_small_df(data, columns):\n",
    "    small_df = data.copy()\n",
    "    small_df = small_df[columns]\n",
    "\n",
    "    return small_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main module with the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM Data Preparation\n",
    "# convert series to supervised learning\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def preprocessing_data(data, n, s_columns):\n",
    "    # normalize features\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(data)\n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled, n, 1)\n",
    "    # drop columns we don't want to predict\n",
    "    columns_to_drop = list(range(-s_columns+1,0))\n",
    "    reframed.drop(reframed.columns[columns_to_drop], axis=1, inplace=True)\n",
    "    values = reframed.values\n",
    "    return scaler, values\n",
    "\n",
    "def split_data(values, n_steps,s_columns, n_train_days, n_valid_days, n_test_days):\n",
    "    train = values[:n_train_days, :]\n",
    "    valid = values[n_train_days : int(n_train_days + n_valid_days), :]\n",
    "    test = values[int(n_train_days + n_valid_days):, :]\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    valid_X, valid_y = valid[:, :-1], valid[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_steps, s_columns))\n",
    "    valid_X = valid_X.reshape((valid_X.shape[0], n_steps, s_columns))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_steps, s_columns))\n",
    "    return train_X, train_y, valid_X, valid_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def summarize_results(scores):\n",
    "    #print(scores)\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    print('Mean %.4f (+/- %.4f)' % (m,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_fit_lstm(train_X, train_y, valid_X, valid_y, test_X, test_y, n_steps, scaler, s_columns):\n",
    "    s = s_columns\n",
    "    n_input = n_steps*s\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='Adagrad', metrics=['mse', 'mae', 'mape'])\n",
    "    history = model.fit(train_X, train_y, epochs=100, verbose=0, batch_size=n_input, shuffle=False, validation_data=(valid_X, valid_y))\n",
    "    # fit network\n",
    "    ###history = model.fit(train_X, train_y, epochs=100, verbose=0, batch_size=n_input, shuffle=False)\n",
    "    # plot history \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history.history['loss'], label='loss_MAE', lw=2)\n",
    "    plt.plot(history.history['val_loss'], label='val_loss_MAE', lw=2)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # evaluate model\n",
    "    results = model.evaluate(test_X, test_y, verbose=0)\n",
    "    loss, mse, mae, mape = results \n",
    "    print('Evaluation results: loss, mse, mae, mape') \n",
    "    print(results)\n",
    "    print ('History results: ')\n",
    "    print('Loss: %.3f - %.3f' % (history.history['loss'][0],history.history['loss'][-1]))\n",
    "    print('Validation Loss: %.3f - %.3f' % (history.history['val_loss'][0],history.history['val_loss'][-1]))\n",
    "   \n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_steps*s_columns))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((yhat, test_X[:, 1-s:]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_y, test_X[:, 1-s:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    # calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    # recalculate MAPE (results are the same actually)\n",
    "    mape = mean_absolute_percentage_error(inv_y, inv_yhat)\n",
    "    #print('Test RMSE: %.3f' % rmse)\n",
    "    #print('Test MAPE: %.3f' % mape)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(inv_yhat, label='yhat', linestyle='--', lw=2)\n",
    "    plt.plot(inv_y, label='y', lw=2)\n",
    "    plt.title('Observed and Predicted Values')\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return loss, rmse, mape\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(n,s,values):\n",
    "    size = values.shape[0]\n",
    "    n_train_days = round(size*0.8)\n",
    "    n_valid_days = round(size*0.1)\n",
    "    #n_valid_days=0\n",
    "    n_test_days = size - n_train_days - n_valid_days\n",
    "    #n_features = 8\n",
    "    n_steps = n\n",
    "    s_columns = s\n",
    "    #Preprocessing\n",
    "    scaler, new_values = preprocessing_data(values, n_steps, s_columns)\n",
    "    #Split data into train and test sets\n",
    "    train_X, train_y, valid_X, valid_y, test_X, test_y = split_data(new_values, n_steps, s_columns, n_train_days, n_valid_days, n_test_days)\n",
    "    #Define and fit our LSTM model\n",
    "    #Make a prediction\n",
    "    loss, rmse, mape = define_fit_lstm(train_X, train_y, valid_X, valid_y, test_X, test_y, n_steps, scaler, s_columns)\n",
    "    return loss, rmse, mape\n",
    "\n",
    "def run_experiment(n,s,values,repeats=5):\n",
    "    print(\"Run experiment with \" + str(repeats) + \" repeats\")\n",
    "   #repeat experiment \n",
    "    losses = list()\n",
    "    rmses = list()\n",
    "    mapes = list()\n",
    "    for r in range(repeats):\n",
    "        print('--------------------------------------------------------------------------------------------------------')\n",
    "        print('Run #%d' % (r+1))\n",
    "        loss, rmse, mape = run_model(n,s,values)\n",
    "        print('>#%d Training Loss: %.3f' % (r+1, loss))\n",
    "        print('>#%d Test RMSE: %.3f' % (r+1, rmse))\n",
    "        print('>#%d Test MAPE: %.3f' % (r+1, mape))\n",
    "        losses.append(loss)\n",
    "        rmses.append(rmse)\n",
    "        mapes.append(mape)\n",
    "    print('--------------------------------------------------------------------------------------------------------')    \n",
    "    print('Final Results: ')\n",
    "    print('Average Loss: ')\n",
    "    summarize_results(losses)\n",
    "    print('Average RMSE: ')\n",
    "    summarize_results(rmses)\n",
    "    print('Average MAPE: ')\n",
    "    summarize_results(mapes)\n",
    "    return(summarize_results(rmses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with 5 repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment with all 21 features\n",
    "vals = df.values\n",
    "vals = vals.astype('float32')\n",
    "run_experiment(2, 21, vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimnets with original data set (8 features), predicting Average sale amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['AVERAGE_SALE_AMOUNT', 'PROFIT', 'NUMBER_OF_SALES','CONVERSION_RATE', 'WEEKDAY', 'COST', 'CLICKS', 'IMPRESSIONS']\n",
    "small_df = create_small_df(df, columns)\n",
    "small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VALUES \n",
    "values = small_df.values\n",
    "values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "for f in df.columns:\n",
    "    series = df[f]\n",
    "    plot_acf(series)\n",
    "    plt.title(f)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experiment(1, 8, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiment(2, 8, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_experiment(3, 8, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with selected features by f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_regression, mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "cols = ['features', 'f_regression', 'mutual_f_regression']\n",
    "score_df = pd.DataFrame([], columns=cols)\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_f_regression(data, k):\n",
    "    array = data.values\n",
    "    X = array[:,1:]\n",
    "    Y = array[:,0]\n",
    "    list_features = data.columns[1:]\n",
    "    columns = data.columns\n",
    "    test = SelectKBest(score_func=f_regression, k=8)\n",
    "    fit = test.fit(X, Y)\n",
    "    # summarize scores\n",
    "    np.set_printoptions(precision=3)\n",
    "    list_scores = fit.scores_\n",
    "    #features = fit.transform(X)\n",
    "    return list_features, list_scores\n",
    "\n",
    "def calc_mutual_f_regression(data, k):\n",
    "    array = data.values\n",
    "    X = array[:,1:]\n",
    "    Y = array[:,0]\n",
    "    list_features = data.columns[1:]\n",
    "    columns = data.columns\n",
    "    test = SelectKBest(score_func=mutual_info_regression, k=8)\n",
    "    fit = test.fit(X, Y)\n",
    "    # summarize scores\n",
    "    np.set_printoptions(precision=3)\n",
    "    list_scores = fit.scores_\n",
    "    #features = fit.transform(X)\n",
    "    return list_features, list_scores\n",
    "\n",
    "#print('#{} {}: {:.2f}'.format(i, columns[i+1], fit.scores_[i]))\n",
    "list_features, list_scores1 = calc_f_regression(data, 8)\n",
    "list_scores2 = calc_mutual_f_regression(data, 8)[1]\n",
    "score_df['features'] = list_features\n",
    "score_df['f_regression'] = list_scores1\n",
    "score_df['mutual_f_regression'] = list_scores2\n",
    "#score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.sort_values('f_regression',ascending = False).head(7)[['features','f_regression']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_f = score_df.sort_values('f_regression',ascending = False).head(7)[['features','f_regression']]['features'].tolist()\n",
    "columns_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.sort_values('mutual_f_regression',ascending = False).head(7)[['features','mutual_f_regression']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_mutal = score_df.sort_values('mutual_f_regression',ascending = False).head(7)[['features','mutual_f_regression']]['features'].tolist()\n",
    "columns_mutal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with top 7 f_regression features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = create_small_df(df, columns_f)\n",
    "new_df.insert(0, 'AVERAGE_SALE_AMOUNT', df['AVERAGE_SALE_AMOUNT'])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VALUES \n",
    "values = new_df.values\n",
    "values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(1, 8, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experiment(2, 8, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM with top 7 features from mutal_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = create_small_df(df, columns_mutal)\n",
    "new_df2.insert(0, 'AVERAGE_SALE_AMOUNT', df['AVERAGE_SALE_AMOUNT'])\n",
    "new_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VALUES \n",
    "values2 = new_df2.values\n",
    "values2 = values2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(1, 8, values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(2, 8, values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### experiment based on plot_acf results. Drop: EPC, Cost_Sales_Ratio, CPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acf_df = df.drop(['CPO', 'EPC', 'COST_SALES_RATIO'], axis=1)\n",
    "acf_values = acf_df.values.astype('float32')\n",
    "run_experiment(3, 18, acf_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoursive feature elimination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "X = df.drop(['AVERAGE_SALE_AMOUNT'], axis=1)\n",
    "y = df['AVERAGE_SALE_AMOUNT']\n",
    "\n",
    "lm.fit(X, y)\n",
    "a = lm.coef_\n",
    "\n",
    "lstm_results = pd.DataFrame({'features': [], 'reults': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k in range(1,12):\n",
    "    rfe = RFE(lm, k)\n",
    "    fit = rfe.fit(X, y)\n",
    "\n",
    "    columns_rfe = []\n",
    "    # report selected features\n",
    "    print('Selected Features:')\n",
    "    names = df.columns.values[1:]\n",
    "    for i in range(len(fit.support_)):\n",
    "        if fit.support_[i]:\n",
    "            columns_rfe.append(names[i])\n",
    "    df_rfe = create_small_df(df, columns_rfe)\n",
    "    df_rfe.insert(0, 'AVERAGE_SALE_AMOUNT', df['AVERAGE_SALE_AMOUNT'])\n",
    "    values_rfe = df_rfe.values.astype('float32')\n",
    "    print(columns_rfe)\n",
    "    result = run_experiment(2, k+1, values_rfe)\n",
    "    lstm_results = lstm_results.append({'features' : columns_rfe, 'reults': result} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lstm_results.iloc[10]\n",
    "a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set = ['CONVERSION_RATE', 'WEEKDAY', 'COST_SALES_RATIO','NUMBER_OF_SOLD_ITEMS']\n",
    "df_set = create_small_df(df, set)\n",
    "df_set.insert(0, 'AVERAGE_SALE_AMOUNT', df['AVERAGE_SALE_AMOUNT'])\n",
    "values_set = df_set.values.astype('float32')\n",
    "f = df_set.shape[1]\n",
    "run_experiment(2, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check left features. Go by cycle be one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_drop = df.drop(['NUMBER_OF_SALES', 'CONVERSION_RATE', 'WEEKDAY', \n",
    "                   'NUMBER_OF_SOLD_ITEMS', 'NEW_CUSTOMER_COUNT','NEW_CUSTOMER_VALUE',\n",
    "                   'CPC', 'CTR', 'COST_SALES_RATIO','CPO','ROI','AVERAGE_SALE_AMOUNT'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left = list(df_drop.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in left:\n",
    "    set = ['CONVERSION_RATE', 'WEEKDAY', 'COST_SALES_RATIO','NUMBER_OF_SOLD_ITEMS']\n",
    "    set.append(i)\n",
    "    df_set = create_small_df(df, set)\n",
    "    df_set.insert(0, 'AVERAGE_SALE_AMOUNT', df['AVERAGE_SALE_AMOUNT'])\n",
    "    values_set = df_set.values.astype('float32')\n",
    "    f = df_set.shape[1]\n",
    "    run_experiment(2, f, values_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the best combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = ['COST', 'CLICKS', 'EPC', 'SALE_AMOUNT_BEFORE_CANCELLATIONS']\n",
    "for i in new_list:\n",
    "    set = ['CONVERSION_RATE', 'WEEKDAY', 'COST_SALES_RATIO','NUMBER_OF_SOLD_ITEMS', 'IMPRESSIONS']\n",
    "    set.append(i)\n",
    "    print('---------------------------' + str(i) + '--------------------------')\n",
    "    print(set)\n",
    "    df_set = create_small_df(df, set)\n",
    "    df_set.insert(0, 'AVERAGE_SALE_AMOUNT', df['AVERAGE_SALE_AMOUNT'])\n",
    "    values_set = df_set.values.astype('float32')\n",
    "    f = df_set.shape[1]\n",
    "    run_experiment(2, f, values_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check ['CONVERSION_RATE', 'WEEKDAY', â€˜COST_SALES_RATIO','NUMBER_OF_SOLD_ITEMS','IMPRESSIONS'] again\n",
    "#### 5-time run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set = ['CONVERSION_RATE', 'WEEKDAY', 'COST_SALES_RATIO','NUMBER_OF_SOLD_ITEMS', 'IMPRESSIONS']\n",
    "df_set = create_small_df(df, set)\n",
    "df_set.insert(0, 'AVERAGE_SALE_AMOUNT', df['AVERAGE_SALE_AMOUNT'])\n",
    "values_set = df_set.values.astype('float32')\n",
    "f = df_set.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-1 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(1, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-2 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experiment(2, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-3 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experiment(3, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-4 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_experiment(4, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-5 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(5, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-7 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(7, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-9 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(9, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-10 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(10, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-12 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(12, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-16 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(16, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-20 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(20, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-50 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(50, f, values_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### t-100 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
