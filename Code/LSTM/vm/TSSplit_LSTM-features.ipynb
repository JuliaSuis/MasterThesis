{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series\n",
    "\n",
    "import math\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import statsmodels\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 25, 20\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from keras.regularizers import l1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load file, print info and select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to load files\n",
    "def load_file(filepath):\n",
    "    df = pd.read_csv(filepath, sep='\\t', index_col=0, parse_dates=True)\n",
    "    df = df.sort_index()\n",
    "    #we can check that this 2 columns are equal, so we can drop one\n",
    "    #any(df['SALE_AMOUNT_BEFORE_CANCELLATIONS'] != df['SALE_AMOUNT_AFTER_CANCELLATIONS'])\n",
    "    df = df.drop(['SALE_AMOUNT_AFTER_CANCELLATIONS'], axis=1)\n",
    "#...\n",
    "    return df.astype('float32')\n",
    "\n",
    "def load_file1(filepath):\n",
    "    df = pd.read_csv(filepath, sep=',', index_col=0, parse_dates=True)\n",
    "    df = df.sort_index()\n",
    "    return df.astype('float32')\n",
    "\n",
    "\n",
    "#function to create a new df with selected columns\n",
    "def create_small_df(df, columns):\n",
    "    small_df = df.copy()\n",
    "    small_df = small_df[columns]\n",
    "    return small_df\n",
    "\n",
    "#function to print inf about Data\n",
    "def print_info_df(df, print_columns = False):\n",
    "    #Count period\n",
    "    d1 = df.index[0]\n",
    "    d2 = df.index[-1]\n",
    "    delta = d2 - d1\n",
    "    print('Number of days is ' + str(delta.days) + ' from ' + str(d1) + ' to '+ str(d2))\n",
    "    print('The shape of the data: %d*%d' %(df.shape[0],df.shape[1]))\n",
    "    print('Check for Nan values: %s'%(df.isnull().values.any()))\n",
    "    if (print_columns == True):\n",
    "        print(list(df.columns))\n",
    "    else:\n",
    "        print('Number of columns: %d'%(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = load_file('..')\n",
    "print_info_df(df, False)\n",
    "feature = 'SALE_AMOUNT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hol = load_file1('../..')\n",
    "df_hol_sel = load_file1('../..')\n",
    "df_sel = load_file1('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_hol_train = df_hol.head(1642)\n",
    "df_hol_sel_train = df_hol.head(1642)\n",
    "df_sel_train = df_hol.head(1642)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    epsilon = 0.000001\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "def root_mean_square_error(y_true, y_pred):\n",
    "    #y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def f_relative_rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    rmse = root_mean_square_error(y_true, y_pred)\n",
    "    return (rmse / np.mean(y_true))*100\n",
    "\n",
    "#def f_smape(A, F):\n",
    "#    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "\n",
    "def f_smape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    epsilon = 0.1\n",
    "    summ = np.maximum(np.abs(y_true) + np.abs(y_pred) + epsilon, 0.5 + epsilon)\n",
    "    smape = np.abs(y_pred - y_true) / summ * 2.0\n",
    "    return np.mean(smape)\n",
    "\n",
    "def summarize_results(scores):\n",
    "    #print(scores)\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    #print('Mean %.4f (+/- %.4f)' % (m,s))\n",
    "    return m,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM Data Preparation\n",
    "# convert series to supervised learning\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def preprocessing_data(data, n, s_columns):\n",
    "    # normalize features\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled = scaler.fit_transform(data)\n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled, n, 1)\n",
    "    # drop columns we don't want to predict\n",
    "    columns_to_drop = list(range(-s_columns+1,0))\n",
    "    reframed.drop(reframed.columns[columns_to_drop], axis=1, inplace=True)\n",
    "    values = reframed.values\n",
    "    return scaler, values\n",
    "\n",
    "def reshape_ts_data(n_steps, s_columns, train_X, train_y, test_X, test_y):\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_steps, s_columns))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_steps, s_columns))\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def time_split_data(values, n_split=9):\n",
    "#     index=1\n",
    "#     plt.figure(1)\n",
    "#     X = values[:, 1:]\n",
    "#     y = values[:, 0]\n",
    "#     tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "#     for train_index, test_index in tscv.split(X):\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#         print('Train X: (%d,%d), Test X: (%d,%d)' % (X_train.shape[0], X_train.shape[1], X_test.shape[0], X_test.shape[1]))\n",
    "#         print('Train y: (%d), Test y: (%d)' % (y_train.shape[0], y_test.shape[0]))\n",
    "#         plt.subplot(910 + index)\n",
    "#         plt.plot(y_train)\n",
    "#         plt.plot([None for i in y_train] + [x for x in y_test])\n",
    "#         index += 1\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# values = df.values\n",
    "# time_split_data(values, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_fit_lstm(train_X, train_y, test_X, test_y, n_steps, scaler, s_columns):\n",
    "    s = s_columns\n",
    "    n_input = n_steps*s\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='Adagrad', metrics=['mse', 'mae', 'mape'])\n",
    "    history = model.fit(train_X, train_y, epochs=100, verbose=0, batch_size=n_input, shuffle=False,\n",
    "                        validation_data=(test_X, test_y))\n",
    "    # fit network\n",
    "    ###history = model.fit(train_X, train_y, epochs=100, verbose=0, batch_size=n_input, shuffle=False)\n",
    "    # plot history \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history.history['loss'], label='loss_MAE', lw=2)\n",
    "    plt.plot(history.history['val_loss'], label='val_loss_MAE', lw=2)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # evaluate model\n",
    "    results = model.evaluate(test_X, test_y, verbose=0)\n",
    "    loss, mse, mae, mape = results \n",
    "    print('loss=%.3f, mse=%.3f, mae=%.3f, mape=%.3f' %(loss, mse, mae, mape)) \n",
    "    print ('History results: ')\n",
    "    print('Loss: %.3f - %.3f' % (history.history['loss'][0],history.history['loss'][-1]))\n",
    "    print('Validation Loss: %.3f - %.3f' % (history.history['val_loss'][0],history.history['val_loss'][-1]))\n",
    "   \n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_steps*s_columns))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((yhat, test_X[:, 1-s:]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_y, test_X[:, 1-s:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    \n",
    "    # calculate RMSE\n",
    "    rmse = root_mean_square_error(inv_y, inv_yhat)\n",
    "    rel_rmse = f_relative_rmse(inv_y, inv_yhat)\n",
    "    # recalculate MAPE (results are the same actually)\n",
    "    mape = mean_absolute_percentage_error(inv_y, inv_yhat)\n",
    "    #print('Test RMSE: %.3f' % rmse)\n",
    "    #print('Test MAPE: %.3f' % mape)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(inv_yhat, label='yhat', linestyle='--', lw=2)\n",
    "    plt.plot(inv_y, label='y', lw=2)\n",
    "    plt.title('Observed and Predicted Values')\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.grid(True)\n",
    "    plt.show() \n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(inv_yhat[:30], label='yhat', linestyle='--', lw=2)\n",
    "    plt.plot(inv_y[:30], label='y', lw=2)\n",
    "    plt.title('Observed and Predicted Values')\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.grid(True)\n",
    "    plt.show() \n",
    "    \n",
    "    return loss, rmse, mape, rel_rmse\n",
    "\n",
    "def run_time_split_data(values, n_steps, s_columns, scaler, n_split):\n",
    "    X = values[:, :-1]\n",
    "    y = values[:, -1]\n",
    "    #new = np.c_[X, y]  \n",
    "    k=1\n",
    "    losses = list()\n",
    "    rmses = list()\n",
    "    mapes = list()\n",
    "    relative_rmses = list()\n",
    "    \n",
    "    skip = 1\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        if (skip > 8):\n",
    "            print(' ')\n",
    "            print('------------------------------------%d-------------------------------------------' %(k))\n",
    "            k+=1\n",
    "            train_X, test_X = X[train_index], X[test_index]\n",
    "            train_y, test_y = y[train_index], y[test_index]\n",
    "            print('Train X: (%d,%d), Test X: (%d,%d)' % (train_X.shape[0], train_X.shape[1],\n",
    "                                                         test_X.shape[0], test_X.shape[1]))\n",
    "            print('Train y: (%d), Test y: (%d)' % (train_y.shape[0], test_y.shape[0]))\n",
    "            new_train_X, new_train_y, new_test_X, new_test_y = reshape_ts_data(n_steps, s_columns, \n",
    "                                                                               train_X, train_y, test_X, test_y)\n",
    "            #Define and fit our LSTM model\n",
    "            #Make a prediction\n",
    "            loss, rmse, mape, rel_rmse = define_fit_lstm(new_train_X, new_train_y, new_test_X, new_test_y,\n",
    "                                               n_steps, scaler, s_columns)\n",
    "            print('Training Loss: %.3f' % (loss))\n",
    "            losses.append(loss)\n",
    "            print('Test RMSE: %.3f' % (rmse))\n",
    "            rmses.append(rmse)\n",
    "            print('Relative RMSE: %.3f' % (rel_rmse))\n",
    "            relative_rmses.append(rel_rmse)\n",
    "            print('Test MAPE: %.3f' % (mape))\n",
    "            mapes.append(mape)\n",
    "            print('---------------------------------------')\n",
    "            print('---------------------------------------')\n",
    "        skip = skip+1\n",
    "        \n",
    "    return losses, rmses, mapes, relative_rmses\n",
    "        \n",
    "    \n",
    "def run_model(n_steps, df, n_split=15):\n",
    "    values = df.values.astype('float32')\n",
    "    feature_set = list(df.columns)\n",
    "    s_columns = df.shape[1]\n",
    "    print('Run model')\n",
    "    print('Features set:'+ str(feature_set))\n",
    "    #Preprocessing\n",
    "    scaler, new_values = preprocessing_data(values, n_steps, s_columns)\n",
    "    #Run\n",
    "    losses, rmses, mapes, relative_rmses = run_time_split_data(new_values, n_steps,\n",
    "                                                               s_columns, scaler, n_split)\n",
    "    print('--------------------------------------------------------------------------------------------------------')    \n",
    "    print('Final Average Results: ')\n",
    "    m,s = summarize_results(losses)\n",
    "    print('Loss: %.4f (s=%.4f)' % (m,s))\n",
    "    m,s = summarize_results(rmses)\n",
    "    print('RMSE: %.4f (s=%.4f)' % (m,s))\n",
    "    m,s = summarize_results(relative_rmses)\n",
    "    print('Relative RMSE: %.4f (s=%.4f)' % (m,s))\n",
    "    m,s = summarize_results(mapes)\n",
    "    print('MAPE: %.4f (s=%.4f)' % (m,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(4, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_hol_train = df_hol.head(1642)\n",
    "# df_hol_sel_train = df_hol.head(1642)\n",
    "# df_sel_train = df_hol.head(1642)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model(2, df_hol_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(3, df_hol_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model(4, df_hol_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(5, df_hol_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model(6, df_hol_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(9, df_hol_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sel_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model(2, df_hol_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(3, df_hol_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model(4, df_hol_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(5, df_hol_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(6, df_hol_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(7, df_hol_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with selected only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_model(2, df_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(3, df_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(4, df_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(5, df_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(6, df_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(7, df_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_model(8, df_sel_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
