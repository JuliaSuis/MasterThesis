{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.Series\n",
    "\n",
    "import math\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "import statsmodels\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 25, 20\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from keras.regularizers import l1\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# import BatchNormalization\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load file, print info and select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to load files\n",
    "def load_file(filepath):\n",
    "    df = pd.read_csv(filepath, sep='\\t', index_col=0, parse_dates=True)\n",
    "    df = df.sort_index()\n",
    "    #we can check that this 2 columns are equal, so we can drop one\n",
    "    #any(df['SALE_AMOUNT_BEFORE_CANCELLATIONS'] != df['SALE_AMOUNT_AFTER_CANCELLATIONS'])\n",
    "    df = df.drop(['SALE_AMOUNT_AFTER_CANCELLATIONS'], axis=1)\n",
    "#...\n",
    "    return df.astype('float32')\n",
    "\n",
    "#function to create a new df with selected columns\n",
    "def create_small_df(df, columns):\n",
    "    small_df = df.copy()\n",
    "    small_df = small_df[columns]\n",
    "    return small_df\n",
    "\n",
    "#function to print inf about Data\n",
    "def print_info_df(df, print_columns = False):\n",
    "    #Count period\n",
    "    d1 = df.index[0]\n",
    "    d2 = df.index[-1]\n",
    "    delta = d2 - d1\n",
    "    print('Number of days is ' + str(delta.days) + ' from ' + str(d1) + ' to '+ str(d2))\n",
    "    print('The shape of the data: %d*%d' %(df.shape[0],df.shape[1]))\n",
    "    print('Check for Nan values: %s'%(df.isnull().values.any()))\n",
    "    if (print_columns == True):\n",
    "        print(list(df.columns))\n",
    "    else:\n",
    "        print('Number of columns: %d'%(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_file('...')\n",
    "print_info_df(df, False)\n",
    "\n",
    "feature = 'SALE_AMOUNT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def root_mean_square_error(y_true, y_pred):\n",
    "    #y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def relative_rmse(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    rmse = root_mean_square_error(y_true, y_pred)\n",
    "    return (rmse / np.mean(y_true))*100\n",
    "\n",
    "#def f_smape(A, F):\n",
    "#    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n",
    "\n",
    "def f_smape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    epsilon = 0.1\n",
    "    summ = np.maximum(np.abs(y_true) + np.abs(y_pred) + epsilon, 0.5 + epsilon)\n",
    "    smape = np.abs(y_pred - y_true) / summ * 2.0\n",
    "    return np.mean(smape)\n",
    "\n",
    "def summarize_results(scores):\n",
    "    #print(scores)\n",
    "    m, s = np.mean(scores), np.std(scores)\n",
    "    #print('Mean %.4f (+/- %.4f)' % (m,s))\n",
    "    return m,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM Data Preparation\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "def preprocessing_data(data, n, s_columns):\n",
    "    # normalize features\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(data)\n",
    "    # frame as supervised learning\n",
    "    reframed = series_to_supervised(scaled, n, 1)\n",
    "    # drop columns we don't want to predict\n",
    "    columns_to_drop = list(range(-s_columns+1,0))\n",
    "    reframed.drop(reframed.columns[columns_to_drop], axis=1, inplace=True)\n",
    "    values = reframed.values\n",
    "    return scaler, values\n",
    "\n",
    "def split_data(values, n_steps,s_columns, n_train_days, n_test_days):\n",
    "    train = values[:n_train_days, :]\n",
    "    test = values[n_train_days:, :]\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_steps, s_columns))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_steps, s_columns))\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "def difference_seria(dataset, order=1):\n",
    "    diff = list()\n",
    "    for i in range(order, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - order]\n",
    "        diff.append(value)\n",
    "    return np.array(diff)\n",
    "\n",
    "def sergey(last, yhat, order=1):\n",
    "    result = list()\n",
    "    result.append(last)\n",
    "    for i in range(1,len(yhat)+1):\n",
    "        result.append(result[i-1] + yhat[i-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_fit_lstm(train_X, train_y, test_X,\n",
    "                    test_y, n_steps, scaler, s_columns):\n",
    "    s = s_columns\n",
    "    n_input = n_steps*s\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='Adagrad', metrics=['mse', 'mae', 'mape'])\n",
    "\n",
    "    history = model.fit(train_X, train_y, epochs=100, verbose=0,\n",
    "                        batch_size=n_input, shuffle=False,\n",
    "                        validation_data=(test_X, test_y))\n",
    "    # fit network\n",
    "    ###history = model.fit(train_X, train_y, epochs=100, verbose=0, batch_size=n_input, shuffle=False)\n",
    "    # plot history \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history.history['loss'], label='loss_MAE', lw=2)\n",
    "    plt.plot(history.history['val_loss'], label='val_loss_MAE', lw=2)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # evaluate model\n",
    "    results = model.evaluate(test_X, test_y, verbose=0)\n",
    "    loss, mse, mae, mape = results \n",
    "    print('loss=%.3f, mse=%.3f, mae=%.3f, mape=%.3f' %(loss, mse, mae, mape)) \n",
    "    print ('History results: ')\n",
    "    print('Loss: %.3f - %.3f' % (history.history['loss'][0],history.history['loss'][-1]))\n",
    "    print('Validation Loss: %.3f - %.3f' % (history.history['val_loss'][0],history.history['val_loss'][-1]))\n",
    "   \n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_steps*s_columns))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((yhat, test_X[:, 1-s:]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_y, test_X[:, 1-s:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    \n",
    "    # calculate RMSE\n",
    "    rmse = root_mean_square_error(inv_y, inv_yhat)\n",
    "    rel_rmse = relative_rmse(inv_y, inv_yhat)\n",
    "    # recalculate MAPE (results are the same actually)\n",
    "    mape = mean_absolute_percentage_error(inv_y, inv_yhat)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(inv_yhat, label='yhat', linestyle='--', lw=2)\n",
    "    plt.plot(inv_y, label='y', lw=2)\n",
    "    plt.title('Observed and Predicted Values')\n",
    "    plt.legend(prop={'size': 18})\n",
    "    plt.grid(True)\n",
    "    plt.show() \n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(inv_yhat[:30], label='yhat', linestyle='--', lw=2)\n",
    "    plt.plot(inv_y[:30], label='y', lw=2)\n",
    "    plt.title('Observed and Predicted Values')\n",
    "    plt.legend(prop={'size': 18})\n",
    "    plt.grid(True)\n",
    "    plt.show() \n",
    "    \n",
    "    return loss, rmse, mape, rel_rmse\n",
    "\n",
    "def run_model(n,s,values):\n",
    "    size = values.shape[0]\n",
    "    n_train_days = round(size*0.9)\n",
    "    n_test_days = size - n_train_days\n",
    "    n_steps = n\n",
    "    s_columns = s\n",
    "    #Preprocessing\n",
    "    scaler, new_values = preprocessing_data(values, n_steps, s_columns)\n",
    "    #Split data into train and test sets\n",
    "    train_X, train_y, test_X, test_y = split_data(new_values, n_steps, s_columns, n_train_days, n_test_days)\n",
    "    #Define and fit our LSTM model\n",
    "    loss, rmse, mape, rel_rmse = define_fit_lstm(train_X, train_y, test_X, test_y, n_steps, scaler, s_columns)\n",
    "    return loss, rmse, mape, rel_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(n, df, repeats=3):\n",
    "    values = df.values.astype('float32')\n",
    "    feature_set = list(df.columns)\n",
    "    s_columns = df.shape[1]\n",
    "    print(\"Run experiment with \" + str(repeats) + \" repeats\")\n",
    "    print('Features set:'+ str(feature_set))\n",
    "   #repeat experiment \n",
    "    losses = list()\n",
    "    rmses = list()\n",
    "    mapes = list()\n",
    "    relative_rmses = list()\n",
    "    for r in range(repeats):\n",
    "        print('--------------------------------------------------------------------------------------------------------')\n",
    "        print('Run #%d' % (r+1))\n",
    "        loss, rmse, mape, relative_rmse = run_model(n,s_columns,values)\n",
    "        print('>#%d Training Loss: %.3f' % (r+1, loss))\n",
    "        print('>#%d Test RMSE: %.3f' % (r+1, rmse))\n",
    "        print('>#%d Test Relative RMSE: %.3f' % (r+1, relative_rmse))\n",
    "        print('>#%d Test MAPE: %.3f' % (r+1, mape))\n",
    "        losses.append(loss)\n",
    "        rmses.append(rmse)\n",
    "        mapes.append(mape)\n",
    "        relative_rmses.append(relative_rmse)\n",
    "    print('--------------------------------------------------------------------------------------------------------')    \n",
    "    print('Final Average Results: ')\n",
    "    m,s = summarize_results(losses)\n",
    "    print('Loss: %.4f (s=%.4f)' % (m,s))\n",
    "    m,s = summarize_results(rmses)\n",
    "    print('RMSE: %.4f (s=%.4f)' % (m,s))\n",
    "    m,s = summarize_results(relative_rmses)\n",
    "    print('Relative RMSE: %.4f (s=%.4f)' % (m,s))\n",
    "    m,s = summarize_results(mapes)\n",
    "    print('MAPE: %.4f (s=%.4f)' % (m,s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full set data\n",
    "values = df.values.astype('float32')\n",
    "f_set = list(df.columns)\n",
    "n_features = df.shape[1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experiment(1, df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
